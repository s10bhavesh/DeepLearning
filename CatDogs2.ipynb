{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_3_CNN (3).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9wAmkqqHpifI"},"source":["\n","# **The Dogs vs. Cats dataset** \n","**It was made available by Kaggle as part of a computer-vision competition in late 2013. You can download the original dataset from www.kaggle.com/c/dogs-vs-cats/data**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6RqnkYahpzWM"},"source":["This dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed). After downloading and uncompressing it, you’ll create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class."]},{"cell_type":"code","metadata":{"id":"hq_tnpJQqXts","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1582480318881,"user_tz":0,"elapsed":1565,"user":{"displayName":"BHAVESH SONWANI","photoUrl":"","userId":"04650862282455408788"}},"outputId":"5f5738df-0556-48de-a90b-d89015dd661d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HRCyDhqapUCx"},"source":["import os, shutil"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Xi3JVtJRGM8"},"source":["import zipfile\n","with zipfile.ZipFile('/content/drive/My Drive/SEM2/Deep Learning/CatsDogs/Cat-Dog-mini.zip', 'r') as zip_ref:\n","    zip_ref.extractall('/content/drive/My Drive/SEM2/Deep Learning/CatsDogs/Cat-Dog-mini')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ikbtDDI2wz_X"},"source":["**Path to the directory where the original dataset was uncompressed**"]},{"cell_type":"code","metadata":{"id":"pBNNiDyEp9e8"},"source":["original_dataset_dir = '/content/drive/My Drive/SEM2/Deep Learning/CatsDogs/Cat-Dog-mini/Cat-Dog-mini'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GEwVG5Vqw4iP"},"source":["**Directory where you’ll store your smaller dataset**"]},{"cell_type":"code","metadata":{"id":"HO1iUHhPqOE1"},"source":["base_dir = '/content/drive/My Drive/SEM2/Deep Learning/CatsDogs/Cat-Dog-mini/catDog'\n","os.mkdir(base_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M82VLz7Rwr0q"},"source":["**Directories for the training, validation, and test splits**"]},{"cell_type":"code","metadata":{"id":"JE_dlF4TqPLj"},"source":["train_dir = os.path.join(base_dir, 'train')\n","os.mkdir(train_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eo9zi4ztEClG"},"source":["validation_dir = os.path.join(base_dir, 'validation')\n","os.mkdir(validation_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QHTCtuBuEPvD"},"source":["test_dir = os.path.join(base_dir, 'test')\n","os.mkdir(test_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CPoaVjdTw_SR"},"source":["**Directory with training cat pictures**"]},{"cell_type":"code","metadata":{"id":"wN3O-IvExPBQ"},"source":["train_cats_dir = os.path.join(train_dir, 'cats')\n","os.mkdir(train_cats_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MqofCqbKxTHR"},"source":["**Directory with training dog pictures**"]},{"cell_type":"code","metadata":{"id":"QLCuXDuBxTlI"},"source":["train_dogs_dir = os.path.join(train_dir, 'dogs')\n","os.mkdir(train_dogs_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"truLEQdixT5p"},"source":["**Directory with validation cat pictures**"]},{"cell_type":"code","metadata":{"id":"0MOY6ZMSxUGh"},"source":["validation_cats_dir = os.path.join(validation_dir, 'cats')\n","os.mkdir(validation_cats_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-kITPY8WxUSQ"},"source":["**Directory with validation dog pictures**"]},{"cell_type":"code","metadata":{"id":"23FEYtJdxUeX"},"source":["validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n","os.mkdir(validation_dogs_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_o2kp_eVxUow"},"source":["**Directory with test cat pictures**"]},{"cell_type":"code","metadata":{"id":"Vkn4LgRNxUzw"},"source":["test_cats_dir = os.path.join(test_dir, 'cats')\n","os.mkdir(test_cats_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K3gfO3nDxs75"},"source":["**Directory with test dog pictures**"]},{"cell_type":"code","metadata":{"id":"IuV-pHFkxtW6"},"source":["test_dogs_dir = os.path.join(test_dir, 'dogs')\n","os.mkdir(test_dogs_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cj0O6VCBxthp"},"source":["**Copies the first 1,000 cat images to train_cats_dir**"]},{"cell_type":"code","metadata":{"id":"nV5A8xllxtq3"},"source":["fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n","for fname in fnames:\n","  src = os.path.join(original_dataset_dir, fname)\n","  dst = os.path.join(train_cats_dir, fname)\n","  shutil.copyfile(src, dst)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zEJ2Vpiuxtyp"},"source":["**Copies the next 500 cat images to validation_cats_dir**"]},{"cell_type":"code","metadata":{"id":"lOfcCBG8xt56"},"source":["fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n","for fname in fnames:\n","  src = os.path.join(original_dataset_dir, fname)\n","  dst = os.path.join(validation_cats_dir, fname)\n","  shutil.copyfile(src, dst)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cqJYQnvzxuBg"},"source":["**Copies the next 500 cat images to test_cats_dir**"]},{"cell_type":"code","metadata":{"id":"tywmMb4HxuJZ"},"source":["fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n","for fname in fnames:\n","  src = os.path.join(original_dataset_dir, fname)\n","  dst = os.path.join(test_cats_dir, fname)\n","  shutil.copyfile(src, dst)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IJ82PhF-xuSZ"},"source":["**Copies the first 1,000 dog images to train_dogs_dir**"]},{"cell_type":"code","metadata":{"id":"gS_UwFo8xubK"},"source":["fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n","for fname in fnames:\n","  src = os.path.join(original_dataset_dir, fname)\n","  dst = os.path.join(train_dogs_dir, fname)\n","  shutil.copyfile(src, dst)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vHf7GUlazWjv"},"source":["**Copies the next 500 dog images to validation_dogs_dir**"]},{"cell_type":"code","metadata":{"id":"QKU7Qz8HzW1W"},"source":["fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n","for fname in fnames:\n","  src = os.path.join(original_dataset_dir, fname)\n","  dst = os.path.join(validation_dogs_dir, fname)\n","  shutil.copyfile(src, dst)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"If1pUUshzXDe"},"source":["***Copies the next 500 dog images to test_dogs_dir***"]},{"cell_type":"code","metadata":{"id":"s1o7_Tg_zXSW"},"source":["fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n","for fname in fnames:\n","  src = os.path.join(original_dataset_dir, fname)\n","  dst = os.path.join(test_dogs_dir, fname)\n","  shutil.copyfile(src, dst)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rigZlVi5zXde"},"source":["**As a sanity check, let’s count how many pictures are in each training split (train/validation/test):**"]},{"cell_type":"code","metadata":{"id":"Zuc1DCUezXo3"},"source":["print('total training cat images:', len(os.listdir(train_cats_dir)))\n","print('total training dog images:', len(os.listdir(train_dogs_dir)))\n","print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n","print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\n","print('total test cat images:', len(os.listdir(test_cats_dir)))\n","print('total test dog images:', len(os.listdir(test_dogs_dir)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2VIg3m5tzXzP"},"source":["**So we have 2,000 training images, 1,000 validation images, and 1,000 test images. Each split contains the same number of samples from each class: this is a balanced binary-classification problem, which means classification accuracy will be an appropriate measure of success.**\n","# **Building your network**"]},{"cell_type":"markdown","metadata":{"id":"7UpfKBfZrFs9"},"source":["### **Instantiating a small convnet for dogs vs. cats classification**"]},{"cell_type":"code","metadata":{"id":"zUN_hfIwqZCr"},"source":["from keras import layers\n","from keras import models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTOj4mjGrKqr"},"source":["model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='relu',\n","                        input_shape=(150, 150, 3)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WRjWdz1rNp7"},"source":["model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Flatten())\n","model.add(layers.Dense(512, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jl_uL_2ErbWp"},"source":["***Let’s look at how the dimensions of the feature maps change with every successive layer:***"]},{"cell_type":"code","metadata":{"id":"mLIv7-vNrSe8"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HAyErTypuvPT"},"source":["# **Configuring the model for training**\n","**For the compilation step, you’ll go with the RMSprop optimizer. Because you ended the network with a single sigmoid unit, you’ll use binary crossentropy as the loss**"]},{"cell_type":"code","metadata":{"id":"5D9IJMYWrfwM"},"source":["from keras import optimizers\n","model.compile(loss='binary_crossentropy',optimizer=optimizers.RMSprop(lr=1e-4),\n","              metrics=['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6mlZTYUZu8AF"},"source":["# **Data preprocessing**"]},{"cell_type":"markdown","metadata":{"id":"ucOHzu6jvBqG"},"source":["Currently, the data sits on a drive as JPEG files, so the steps for getting it into the network are roughly as follows:\n","1.   Read the picture files.\n","2.   Decode the JPEG content to RGB grids of pixels.\n","3.   Convert these into floating-point tensors.\n","4.   Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values)."]},{"cell_type":"markdown","metadata":{"id":"sf9Fh862vSnG"},"source":["**Using ImageDataGenerator to read images from directories**"]},{"cell_type":"code","metadata":{"id":"XBozKe0Su5A0"},"source":["from keras.preprocessing.image import ImageDataGenerator\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(150, 150),\n","    batch_size=20,\n","    class_mode='binary')\n","validation_generator = test_datagen.flow_from_directory(validation_dir,\n","                                                        target_size=(150, 150),\n","                                                        batch_size=20,\n","                                                        class_mode='binary')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XWHsbmGn2OLh"},"source":["The output of one of these generators: it yields batches of 150 × 150 RGB images (shape (20, 150, 150, 3)) and binary labels (shape (20,))."]},{"cell_type":"markdown","metadata":{"id":"CShfXOA0NIAz"},"source":["There are 20 samples in each batch (the batch size). \n","\n","**Note**: The generator yields these batches indefinitely: it loops endlessly over the images in the target folder. For this reason, you need to break the iteration loop at some point:"]},{"cell_type":"code","metadata":{"id":"pSX4pXKZvi7F"},"source":["for data_batch, labels_batch in train_generator:\n","  print('data batch shape:', data_batch.shape)\n","  print('labels batch shape:', labels_batch.shape)\n","  break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1JJJS5Nr2yxI"},"source":["Let’s fit the model to the data using the generator. You do so using the fit_generator method, the equivalent of fit for data generators like this one. It expects as its first argument a Python generator that will yield batches of inputs and targets indefinitely, like this one does. Because the data is being generated endlessly, the Keras model needs to know how many samples to draw from the generator before declaring an epoch over."]},{"cell_type":"markdown","metadata":{"id":"zNRSGDwUNqCV"},"source":["This is the role of the steps_per_epoch argument: after having drawn steps_per_epoch batches from the generator—that is, after having run for steps_per_epoch gradient descent steps—the fitting process will go to the next epoch. In this case, batches are 20 samples, so it will take 100 batches until you see your target of 2,000 samples."]},{"cell_type":"markdown","metadata":{"id":"x9JElDK9Ns9F"},"source":["When using fit_generator, you can pass a validation_data argument, much as with the fit method. It’s important to note that this argument is allowed to be a data generator, but it could also be a tuple of Numpy arrays. If you pass a generator as validation_data, then this generator is expected to yield batches of validation data endlessly; thus you should also specify the validation_steps argument, which tells the process how many batches to draw from the validation generator for evaluation."]},{"cell_type":"markdown","metadata":{"id":"XyDPBWTx35J6"},"source":["# **Fitting the model using a batch generator**"]},{"cell_type":"code","metadata":{"id":"8YI72x3e2-jh"},"source":["history = model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=100,\n","    epochs=3,\n","    validation_data=validation_generator,\n","    validation_steps=50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EaABVLHg4CcY"},"source":["# **Saving the model**"]},{"cell_type":"code","metadata":{"id":"djk8OmNN3-rr"},"source":["model.save('cats_and_dogs_small_1.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAiR-dBw4HrL"},"source":["## **Displaying curves of loss and accuracy during training**"]},{"cell_type":"code","metadata":{"id":"YI70sZXe4LSk"},"source":["import matplotlib.pyplot as plt\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","epochs = range(1, len(acc) + 1)\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","plt.figure()\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_JU6e6_M40D_"},"source":["These plots are characteristic of overfitting. The training accuracy increases linearly over time, until it reaches nearly 100%, whereas the validation accuracy stalls at 70–72%. The validation loss reaches its minimum after only five epochs and then stalls, whereas the training loss keeps decreasing linearly until it reaches nearly 0. \n","\n","Because you have relatively few training samples (2,000), overfitting will be your number-one concern. You already know about a number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). We’re now going to work with a new one, specific to computer vision and used almost universally when processing images with deep-learning models: data augmentation."]},{"cell_type":"markdown","metadata":{"id":"dTlKV59o4-Xa"},"source":["# **Using data augmentation**\n","Overfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your model would be exposed to every possible aspect of the data distribution at hand: you would never overfit. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better. \n","\n","In Keras, this can be done by configuring a number of random transformations to be performed on the images read by the ImageDataGenerator instance."]},{"cell_type":"markdown","metadata":{"id":"0zgpfFkt5YTC"},"source":["**Setting up a data augmentation configuration via ImageDataGenerator**"]},{"cell_type":"code","metadata":{"id":"MBMH3nP148NN"},"source":["datagen = ImageDataGenerator(\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"plZnpfjD5p5x"},"source":["**Let’s quickly go over this code:**\n","\n","*   **rotation_range** is a value in degrees (0–180), a range within which to randomly rotate pictures.\n","*   **width_shift** and **height_shift** are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\n","*   **shear_range** is for randomly applying shearing transformations.\n","*   **zoom_range** is for randomly zooming inside pictures.\n","*   **horizontal_flip** is for randomly flipping half the images horizontally—relevant when there are no assumptions of horizontal asymmetry (for example, real-world pictures).\n","*   **fill_mode** is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift."]},{"cell_type":"markdown","metadata":{"id":"APO9cwdc6iKT"},"source":["## **Displaying some randomly augmented training images**"]},{"cell_type":"code","metadata":{"id":"eAFYE3qW5lC6"},"source":["from keras.preprocessing import image\n","fnames = [os.path.join(train_cats_dir, fname) for\n","          fname in os.listdir(train_cats_dir)]\n","img_path = fnames[3]\n","img = image.load_img(img_path, target_size=(150, 150))\n","x = image.img_to_array(img)\n","x = x.reshape((1,) + x.shape)\n","i = 0\n","for batch in datagen.flow(x, batch_size=1):\n","  plt.figure(i)\n","  imgplot = plt.imshow(image.array_to_img(batch[0]))\n","  i += 1\n","  if i % 4 == 0:\n","    break\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iH3o5DkqUOml"},"source":["# Defining a new convnet that includes dropout"]},{"cell_type":"code","metadata":{"id":"XhtIXmLxUQJS"},"source":["model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 3)))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Flatten())\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(512, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy',\n","              optimizer=optimizers.RMSprop(lr=1e-4),\n","              metrics=['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ttsWeWfdUcdx"},"source":["# **Training the convnet using data-augmentation generators**"]},{"cell_type":"code","metadata":{"id":"_TcAYLKgUToT"},"source":["train_datagen = ImageDataGenerator(rescale=1./255,\n","                                   rotation_range=40,\n","                                   width_shift_range=0.2,\n","                                   height_shift_range=0.2,\n","                                   shear_range=0.2,\n","                                   zoom_range=0.2,\n","                                   horizontal_flip=True,)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","train_generator = train_datagen.flow_from_directory(train_dir,\n","                                                    target_size=(150, 150),\n","                                                    batch_size=32,\n","                                                    class_mode='binary')\n","validation_generator = test_datagen.flow_from_directory(\n","    validation_dir,\n","    target_size=(150, 150),\n","    batch_size=32,\n","    class_mode='binary')\n","history = model.fit_generator(train_generator,\n","                              steps_per_epoch=100,\n","                              epochs=1,\n","                              validation_data=validation_generator,\n","                              validation_steps=50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J-95qmlJVDri"},"source":["## ***Saving the model***"]},{"cell_type":"code","metadata":{"id":"Xn_D0l6VU2cj"},"source":["model.save('cats_and_dogs_small_2.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bsen2H_AVJPj"},"source":["# **Visualizing what convnets learn**"]},{"cell_type":"markdown","metadata":{"id":"H4O5GC_OVolq"},"source":["### Visualizing intermediate activations"]},{"cell_type":"code","metadata":{"id":"NLXabYtuV2pP"},"source":["from keras.models import load_model\n","model = load_model('cats_and_dogs_small_2.h5')\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"84m6dQzmWj0c"},"source":["**Preprocessing a single image**"]},{"cell_type":"code","metadata":{"id":"nBEPTMT9WSDo"},"source":["img_path = '/content/drive/My Drive/2019/Dubai Lecture/2nd Visit/cats_and_dogs_small/cat.9672.jpg'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"01lK5MItYFD8"},"source":["from keras.preprocessing import image\n","import numpy as np\n","img = image.load_img(img_path, target_size=(150, 150))\n","img_tensor = image.img_to_array(img)\n","img_tensor = np.expand_dims(img_tensor, axis=0)\n","img_tensor /= 255.\n","#Its shape is (1, 150, 150, 3)\n","print(img_tensor.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0U98InFUYVAf"},"source":["**Displaying the test picture**"]},{"cell_type":"code","metadata":{"id":"6zcmWD-nYQAu"},"source":["import matplotlib.pyplot as plt\n","plt.imshow(img_tensor[0])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IXkKnoQ4YmVZ"},"source":["**Instantiating a model from an input tensor and a list of output tensors**"]},{"cell_type":"markdown","metadata":{"id":"KGAQslJCY80h"},"source":["In order to extract the feature maps you want to look at, you’ll create a Keras model that takes batches of images as input, and outputs the activations of all convolution and pooling layers. \n","\n","To do this, you’ll use the Keras class Model. A model is instantiated using two arguments: an input tensor (or list of input tensors) and an output tensor (or list of output tensors). \n","\n","The resulting class is a Keras model, just like the Sequential models you’re familiar with, mapping the specified inputs to the specified outputs."]},{"cell_type":"code","metadata":{"id":"ULwmhd_0Yarq"},"source":["from keras import models\n","layer_outputs = [layer.output for layer in model.layers[:8]]\n","activation_model = models.Model(inputs=model.input, outputs=layer_outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GDmgSE4bZchj"},"source":["When fed an image input, this model returns the values of the layer activations in the original model. \n","\n","This is the first time you’ve encountered a multi-output model in this\n","book: until now, the models you’ve seen have had exactly one input and one output.\n","In the general case, a model can have any number of inputs and outputs. This one has\n","one input and eight outputs: one output per layer activation"]},{"cell_type":"code","metadata":{"id":"gU1bD7aOZIqu"},"source":["#Returns a list of five Numpy arrays: one array per layer activation\n","activations = activation_model.predict(img_tensor)\n","print(activations)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HjgBIbPvZo65"},"source":["For instance, this is the activation of the first convolution layer for the cat image input:"]},{"cell_type":"code","metadata":{"id":"Ram9hiU1Zyev"},"source":["first_layer_activation = activations[0]\n","print(first_layer_activation.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iZYudTztaFCx"},"source":["It’s a 148 × 148 feature map with 32 channels. \n","\n","Let’s try plotting the fourth channel of the activation of the first layer of the original model"]},{"cell_type":"markdown","metadata":{"id":"LOZvr-KCaMRZ"},"source":["# Visualizing the fourth channel"]},{"cell_type":"code","metadata":{"id":"pY5RpH9FaBkk"},"source":["import matplotlib.pyplot as plt\n","plt.matshow(first_layer_activation[0, :, :, 31], cmap='viridis')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-FzOnbXa48i"},"source":["# **Visualizing every channel in every intermediate activation**"]},{"cell_type":"code","metadata":{"id":"uSgcfDejaH3R"},"source":["layer_names = []\n","# Names of the layers, so you can have them as part of your plot\n","for layer in model.layers[:8]:\n","  layer_names.append(layer.name)\n","\n","images_per_row = 16\n","\n","# Displays the feature maps\n","for layer_name, layer_activation in zip(layer_names, activations):\n","  # Number of features in the feature map\n","  n_features = layer_activation.shape[-1]\n","\n","  # The feature map has shape (1, size, size, n_features).\n","  size = layer_activation.shape[1]\n","  \n","  # Tiles the activation channels in this matrix\n","  n_cols = n_features // images_per_row\n","  display_grid = np.zeros((size * n_cols, images_per_row * size))\n","  \n","  # Tiles each filter into a big horizontal grid\n","  for col in range(n_cols):\n","    for row in range(images_per_row):\n","      channel_image = layer_activation[0,:, :,\n","                                       col * images_per_row + row]\n","      \n","      # Post-processes the feature to make it visually palatable\n","      channel_image -= channel_image.mean()\n","      channel_image /= channel_image.std()\n","      channel_image *= 64\n","      channel_image += 128\n","      channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n","      \n","      # Displays the grid\n","      display_grid[col * size : (col + 1) * size,\n","                   row * size : (row + 1) * size] = channel_image\n","  scale = 1. / size\n","  plt.figure(figsize=(scale * display_grid.shape[1],\n","                      scale * display_grid.shape[0]))\n","  plt.title(layer_name)\n","  plt.grid(False)\n","  plt.imshow(display_grid, aspect='auto', cmap='viridis')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1eg3aQQWbrsI"},"source":[""],"execution_count":null,"outputs":[]}]}